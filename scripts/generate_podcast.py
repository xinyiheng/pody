import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import json
import edge_tts
import asyncio
import os
from typing import List, Dict
import shutil

class PodcastGenerator:
    def __init__(self):
        self.rss_url = "https://www.inoreader.com/stream/user/1005507650/tag/%E5%9B%BD%E5%86%85%E5%87%BA%E7%89%88%E5%95%86%E5%85%AC%E4%BC%97%E5%8F%B7"
        # ä»ç¯å¢ƒå˜é‡è·å– API key
        self.api_key = os.environ.get('API_KEY')
        if not self.api_key:
            raise ValueError("API_KEY environment variable is not set")
        self.api_base = "https://openrouter.ai/api/v1/chat/completions"
        self.cache_file = "article_cache.json"
        self.progress_file = "process_progress.json"
        self.web_dir = 'web'
        self.public_dir = os.path.join(self.web_dir, 'public')
        self.podcasts_dir = os.path.join(self.public_dir, 'podcasts')
        self.index_file = os.path.join(self.web_dir, 'podcast_index.json')
        
        # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
        for directory in [self.web_dir, self.public_dir, self.podcasts_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory)

    def load_cache(self) -> Dict:
        """åŠ è½½æ–‡ç« ç¼“å­˜ï¼Œå¹¶æ¸…ç†è¿‡æœŸå†…å®¹"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                    
                # æ¸…ç†7å¤©å‰çš„ç¼“å­˜
                current_time = datetime.now()
                cleaned_cache = {'articles': {}}
                
                for url, article_data in cache['articles'].items():
                    try:
                        article_time = datetime.strptime(article_data['timestamp'], '%Y-%m-%d %H:%M:%S')
                        if (current_time - article_time).days < 7:
                            cleaned_cache['articles'][url] = article_data
                    except:
                        continue
                
                return cleaned_cache
            return {'articles': {}}
        except Exception as e:
            print(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return {'articles': {}}

    def save_cache(self, cache: Dict):
        """ä¿å­˜æ–‡ç« ç¼“å­˜"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def update_podcast_index(self, podcast_data: Dict) -> None:
        """æ›´æ–°æ’­å®¢ç´¢å¼•æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.index_file), exist_ok=True)
            
            if os.path.exists(self.index_file):
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    index = json.load(f)
                    print(f"å½“å‰ç´¢å¼•åŒ…å« {len(index['podcasts'])} ä¸ªæ’­å®¢")
                    print(f"ç´¢å¼•æ–‡ä»¶ä½ç½®: {self.index_file}")
            else:
                index = {"podcasts": []}
                print(f"åˆ›å»ºæ–°çš„ç´¢å¼•æ–‡ä»¶: {self.index_file}")
            
            # å°†æ–°æ’­å®¢æ·»åŠ åˆ°åˆ—è¡¨å¼€å¤´
            index["podcasts"].insert(0, podcast_data)
            print(f"æ·»åŠ æ–°æ’­å®¢: {podcast_data['id']}")
            print(f"éŸ³é¢‘æ–‡ä»¶è·¯å¾„: {os.path.join(self.podcasts_dir, podcast_data['id'], 'podcast.mp3')}")
            
            # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(index, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ç´¢å¼•æ–‡ä»¶å·²æ›´æ–°: {self.index_file}")
            print(f"ç°åœ¨ç´¢å¼•åŒ…å« {len(index['podcasts'])} ä¸ªæ’­å®¢")
            
        except Exception as e:
            print(f"æ›´æ–°ç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            print(traceback.format_exc())

    async def generate_broadcast_script(self, summaries: List[Dict]) -> str:
        """ç”Ÿæˆå•äººæ’­æŠ¥ç¨¿"""
        try:
            valid_summaries = [s for s in summaries if s.get('summary') and s['summary'].strip()]
            
            if not valid_summaries:
                print("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ç« å†…å®¹å¯ä»¥è®¨è®º")
                return ""
            
            article_count = len(valid_summaries)
            input_text = "\n\n".join([
                f"æ–‡ç« {i+1}:\næ ‡é¢˜: {s['title']}\næ¥æº: {s['source']}\næ€»ç»“:\n{s['summary']}"
                for i, s in enumerate(valid_summaries)
            ])
            
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ’­å®¢å†…å®¹åˆ›ä½œè€…ï¼Œæ“…é•¿å°†å¤šç¯‡æ–‡ç« æ•´åˆæˆè‡ªç„¶æµç•…çš„å•äººå£æ’­ç¨¿ä»¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹{article_count}ç¯‡æ–‡ç« ï¼Œç”Ÿæˆä¸€æœŸå‡ºç‰ˆè¡Œä¸šæ–°é—»æ’­æŠ¥ã€‚

å†…å®¹ææ–™ï¼ˆå…±{article_count}ç¯‡æ–‡ç« ï¼‰ï¼š
{input_text}

å…·ä½“è¦æ±‚ï¼š

1. å†…å®¹ç»„ç»‡
   - æŒ‰ä¸»é¢˜ï¼ˆå¦‚æ–‡å­¦ã€ç«¥ä¹¦ã€å•†ä¸šã€æ•°å­—å‡ºç‰ˆç­‰ï¼‰å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»æ•´ç†
   - ç›¸ä¼¼ä¸»é¢˜çš„å†…å®¹æ”¾åœ¨ä¸€èµ·è®¨è®ºï¼Œä½¿ç”¨è‡ªç„¶çš„è¿‡æ¸¡å¥è¿æ¥
   - æ¯ä¸ªä¸»é¢˜ä¸‹çš„å†…å®¹è¦çªå‡ºé‡ç‚¹ï¼Œå±•ç°æ·±åº¦

2. æ¥æºå¼•ç”¨
   - æ¯ç¯‡æ–‡ç« å¿…é¡»å‡†ç¡®æåŠå…¶çœŸå®æ¥æº
   - æ¥æºå¼•ç”¨è¦è‡ªç„¶èå…¥è¯­å¥ï¼Œé¿å…ç”Ÿç¡¬å †ç Œ

3. è¯­è¨€é£æ ¼
   - ä¿æŒä¸“ä¸šæ€§å’Œæƒå¨æ„Ÿï¼ŒåŒæ—¶è¯­è¨€è¦ç”ŸåŠ¨æ˜“æ‡‚
   - ä½¿ç”¨å¹¿æ’­æ–°é—»çš„è¯­æ°”å’ŒèŠ‚å¥
   - é¿å…è¿‡äºå£è¯­åŒ–çš„è¡¨è¾¾

4. ç»“æ„è¦æ±‚
   - ä¸è¦æ·»åŠ å¼€åœºç™½å’Œç»“æŸè¯­
   - æ¯ç¯‡æ–‡ç« è®¨è®ºç¯‡å¹…300å­—å·¦å³
   - ç¡®ä¿è¦†ç›–æ‰€æœ‰æ–‡ç« çš„æ ¸å¿ƒå†…å®¹
   - çªå‡ºè¡Œä¸šå½±å“å’Œæ·±å±‚åˆ†æ

è¯·ç›´æ¥è¾“å‡ºæ’­æŠ¥å†…å®¹ï¼Œä»¥å¹¿æ’­æ–°é—»çš„ä¸“ä¸šé£æ ¼å‘ˆç°ã€‚"""

            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "HTTP-Referer": "https://github.com/",
                "Content-Type": "application/json"
            }

            payload = {
                "model": "qwen/qwen-turbo",
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯å‡ºç‰ˆç”µå°çš„ä¸“ä¸šä¸»æ’­ï¼Œæ“…é•¿åˆ¶ä½œæ–°é—»æ’­æŠ¥å†…å®¹ã€‚"
                    },
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "top_p": 0.9
            }

            response = requests.post(
                self.api_base,
                headers=headers,
                json=payload,
                timeout=180  # è¿›ä¸€æ­¥å¢åŠ è¶…æ—¶æ—¶é—´åˆ°3åˆ†é’Ÿ
            )
            
            response.raise_for_status()
            script = response.json()["choices"][0]["message"]["content"]
            
            # éªŒè¯æ˜¯å¦åŒ…å«æ‰€æœ‰æ–‡ç« 
            for summary in valid_summaries:
                if summary['source'] not in script:
                    print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°æ¥æº '{summary['source']}' çš„å†…å®¹")
            
            # ä¿å­˜æ–‡ç¨¿
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            podcast_dir = os.path.join(self.podcasts_dir, timestamp)
            if not os.path.exists(podcast_dir):
                os.makedirs(podcast_dir)
            
            script_file = os.path.join(podcast_dir, 'script.txt')
            with open(script_file, 'w', encoding='utf-8') as f:
                f.write(script)
            
            print(f"\næ’­æŠ¥ç¨¿å·²ç”Ÿæˆå¹¶ä¿å­˜åˆ°: {script_file}")
            return script
            
        except requests.exceptions.RequestException as e:
            print(f"API è¯·æ±‚å¤±è´¥: {str(e)}")
            if hasattr(e.response, 'text'):
                print(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
            return ""

    async def generate_audio(self, text: str, timestamp: str) -> str:
        """ä½¿ç”¨Edge TTSç”ŸæˆéŸ³é¢‘"""
        print("å¼€å§‹ç”ŸæˆéŸ³é¢‘...")
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            podcast_dir = os.path.join(self.podcasts_dir, timestamp)
            if not os.path.exists(podcast_dir):
                os.makedirs(podcast_dir)
            
            communicate = edge_tts.Communicate(
                text, 
                "zh-CN-XiaoxiaoNeural",
                rate="+50%"
            )
            
            audio_file = os.path.join(podcast_dir, 'podcast.mp3')
            await communicate.save(audio_file)
            
            print(f"âœ… éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜åˆ°: {audio_file}")
            return audio_file
        except Exception as e:
            print(f"ç”ŸæˆéŸ³é¢‘å¤±è´¥: {e}")
            return None

    def fetch_article_content(self, url, max_retries=3):
        """è·å–æ–‡ç« å…¨æ–‡å†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
        print(f"\næ­£åœ¨å¤„ç†URL: {url}")
        
        for attempt in range(max_retries):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Connection': 'keep-alive'
                }
                
                if 'mp.weixin.qq.com' in url:
                    response = requests.get(url, headers=headers, timeout=30)
                    response.encoding = 'utf-8'
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    article = soup.find('div', id='js_content')
                    
                    if article:
                        # æ¸…ç†æ–‡ç« å†…å®¹
                        for tag in article.find_all(True):
                            if 'style' in tag.attrs:
                                del tag.attrs['style']
                        
                        for unwanted in article.find_all(['script', 'style', 'iframe']):
                            unwanted.decompose()
                        
                        # è·å–æ–‡æœ¬æ®µè½
                        paragraphs = []
                        seen_texts = set()
                        
                        for element in article.find_all(['p', 'section', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                            text = ' '.join(element.get_text(strip=True).split())
                            
                            if (text and 
                                len(text) > 2 and
                                not any(text.startswith(x) for x in [
                                    'å¾®ä¿¡', 'å›¾ç‰‡', 'â—', 'Â©', '=', '...', 'ã€Œ', 'æ ¡å¯¹', 
                                    'ç¼–è¾‘', 'å¤å®¡', 'ç»ˆå®¡', 'æ¨è', 'é˜…è¯»', 'ç‚¹å‡»', 'å…³æ³¨',
                                    'æ¥æº', 'ä½œè€…', 'æ ‡é¢˜', 'å‘å¸ƒæ—¶é—´', 'åŸæ–‡é“¾æ¥', 'å†…å®¹'
                                ]) and
                                not any(x in text for x in [
                                    'æ‰«æäºŒç»´ç ', 'é•¿æŒ‰å›¾ç‰‡', 'ç‚¹å‡»ä¸Šæ–¹', 'å…³æ³¨æˆ‘ä»¬', 
                                    'æ–°åª’ä½“çŸ©é˜µ', 'åŸåˆ›æ–‡ç« ', 'æ¬¢è¿è½¬å‘', 'æœ‹å‹åœˆ'
                                ])):
                                
                                is_substring = False
                                for seen_text in seen_texts:
                                    if text in seen_text or seen_text in text:
                                        is_substring = True
                                        break
                                
                                if not is_substring and text not in seen_texts:
                                    seen_texts.add(text)
                                    paragraphs.append(text)
                        
                        content = '\n\n'.join(paragraphs)
                        
                        if len(content) > 100:
                            print(f"æˆåŠŸè·å–æ–‡ç« å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
                            return content
                        
                        if attempt < max_retries - 1:
                            print(f"å†…å®¹å¤ªçŸ­ï¼Œå°è¯•é‡æ–°è·å– (å°è¯• {attempt + 2}/{max_retries})")
                            time.sleep(3)  # ç­‰å¾…å‡ ç§’åé‡è¯•
                            continue
                    
                    if attempt < max_retries - 1:
                        print(f"æœªæ‰¾åˆ°æ–‡ç« å†…å®¹ï¼Œå°è¯•é‡æ–°è·å– (å°è¯• {attempt + 2}/{max_retries})")
                        time.sleep(3)
                        continue
                    else:
                        print("å¤šæ¬¡å°è¯•åä»æœªè·å–åˆ°æœ‰æ•ˆå†…å®¹")
                        return "æ— æ³•è·å–æ–‡ç« å†…å®¹"
                    
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"è·å–å¤±è´¥ï¼Œå°è¯•é‡æ–°è·å– (å°è¯• {attempt + 2}/{max_retries}): {e}")
                    time.sleep(3)
                    continue
                else:
                    print(f"å¤šæ¬¡å°è¯•åè·å–å¤±è´¥: {e}")
                    return f"è·å–æ–‡ç« å†…å®¹å¤±è´¥: {str(e)}"
        
        return "æ— æ³•è·å–æ–‡ç« å†…å®¹"

    def fetch_rss_articles(self, num_pages=5):
        """è·å–RSSæ–‡ç« åˆ—è¡¨ï¼Œæ”¯æŒå¤šé¡µè·å–å’Œå»é‡"""
        try:
            print("å¼€å§‹è·å–RSSæ–‡ç« ...")
            
            articles = []
            cache = self.load_cache()
            
            # è·å–å·²å¤„ç†çš„URLå’Œå®ƒä»¬çš„æ—¶é—´æˆ³
            processed_urls = {}
            for url, data in cache['articles'].items():
                try:
                    timestamp = datetime.strptime(data['timestamp'], '%Y-%m-%d %H:%M:%S')
                    processed_urls[url] = timestamp
                except:
                    continue
            
            seen_urls = set()
            
            for page in range(num_pages):
                page_url = f"{self.rss_url}?n=20&p={page + 1}"
                print(f"\nè·å–ç¬¬ {page + 1} é¡µ...")
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'application/rss+xml,application/xml;q=0.9,*/*;q=0.8'
                }
                
                response = requests.get(page_url, headers=headers, timeout=30)
                feed = feedparser.parse(response.text)
                
                if not feed.entries:
                    print(f"ç¬¬ {page + 1} é¡µæ²¡æœ‰æ›´å¤šæ–‡ç« ")
                    break
                    
                print(f"æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")
                
                for entry in feed.entries:
                    try:
                        # æ£€æŸ¥æ˜¯å¦åœ¨å½“å‰è¿è¡Œä¸­å·²å¤„ç†
                        if entry.link in seen_urls:
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨ç¼“å­˜ä¸­ä¸”æœªè¿‡æœŸ
                        if entry.link in processed_urls:
                            cache_time = processed_urls[entry.link]
                            if (datetime.now() - cache_time).days < 7:
                                print(f"è·³è¿‡æœ€è¿‘å¤„ç†çš„æ–‡ç« : {entry.get('title', 'No title')}")
                                continue
                        
                        seen_urls.add(entry.link)
                        
                        print(f"\nå¤„ç†æ–‡ç« : {entry.get('title', 'No title')}")
                        
                        author = entry.get('dc_creator', 'æœªçŸ¥ä½œè€…')
                        source = entry.get('source', {}).get('title', 'æœªçŸ¥æ¥æº')
                        
                        content = self.fetch_article_content(entry.link)
                        if content and content != "æ— æ³•è·å–æ–‡ç« å†…å®¹":
                            article = {
                                'title': entry.title,
                                'author': author,
                                'source': source,
                                'link': entry.link,
                                'pub_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                'content': content
                            }
                            articles.append(article)
                            
                            # æ›´æ–°ç¼“å­˜
                            cache['articles'][entry.link] = {
                                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                'data': article
                            }
                            
                            print(f"æˆåŠŸæ·»åŠ æ–‡ç« : {entry.title}")
                            
                            if len(articles) >= 100:  # é™åˆ¶æœ€å¤§æ–‡ç« æ•°
                                print("\nå·²è¾¾åˆ°æœ€å¤§æ–‡ç« æ•°é™åˆ¶(100)")
                                break
                        
                        time.sleep(2)  # é¿å…é¢‘ç¹è¯·æ±‚
                        
                    except Exception as e:
                        print(f"å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                        continue
                
                if len(articles) >= 100:
                    break
                
                time.sleep(3)  # é¡µé¢ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
            
            # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
            self.save_cache(cache)
            
            print(f"\næˆåŠŸè·å– {len(articles)} ç¯‡æ–°æ–‡ç« ")
            return articles
            
        except Exception as e:
            print(f"è·å–RSSæ–‡ç« å¤±è´¥: {e}")
            import traceback
            print(traceback.format_exc())
            return []

    def summarize_with_ai(self, articles: List[Dict]) -> List[Dict]:
        summaries = []
        for article in articles:
            try:
                print(f"\næ­£åœ¨æ€»ç»“æ–‡ç« : {article['title']}")
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                # æ‰“å°è¯·æ±‚ä¿¡æ¯ï¼ˆæ³¨æ„éšè—å®Œæ•´ API keyï¼‰
                print(f"API Key å‰ç¼€: {self.api_key[:10]}...")
                print(f"è¯·æ±‚ URL: {self.api_base}")
                
                response = requests.post(
                    self.api_base,
                    headers=headers,
                    json={
                        "model": "anthropic/claude-3-opus-20240229",
                        "messages": [
                            {"role": "user", "content": f"è¯·å°†è¿™ç¯‡æ–‡ç« æ€»ç»“ä¸ºé€‚åˆæ’­å®¢çš„å†…å®¹ï¼Œè¯­æ°”è¦è‡ªç„¶æµç•…ï¼Œè¦åŒ…å«æ–‡ç« çš„ä¸»è¦è§‚ç‚¹å’Œæœ‰è¶£çš„ç»†èŠ‚ã€‚\n\næ–‡ç« æ ‡é¢˜ï¼š{article['title']}\n\nä½œè€…ï¼š{article['author']}\n\nå†…å®¹ï¼š{article['content']}"}
                        ]
                    },
                    timeout=60
                )
                
                # æ‰“å°å“åº”çŠ¶æ€å’Œå†…å®¹
                print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                if response.status_code != 200:
                    print(f"é”™è¯¯å“åº”: {response.text}")
                
                response.raise_for_status()
                summary = {
                    'title': article['title'],
                    'author': article['author'],
                    'source': article['source'],
                    'summary': response.json()["choices"][0]["message"]["content"]
                }
                summaries.append(summary)
                
                # é¿å…é¢‘ç¹è¯·æ±‚
                time.sleep(2)
            except Exception as e:
                print(f"AIæ€»ç»“å¤±è´¥: {str(e)}")
                continue
        
        return summaries

async def main():
    generator = PodcastGenerator()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. è·å–æ–‡ç« 
    articles = generator.fetch_rss_articles()
    if not articles:
        print("æœªè·å–åˆ°æ–‡ç« ")
        return
    
    print(f"\næˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« :")
    for i, article in enumerate(articles, 1):
        print(f"{i}. {article['title']} ({len(article['content'])} å­—)")
    
    # 2. AIæ€»ç»“
    summaries = generator.summarize_with_ai(articles)
    if not summaries:
        print("AIæ€»ç»“å¤±è´¥")
        return
    
    # 3. ç”Ÿæˆæ’­æŠ¥ç¨¿
    script = await generator.generate_broadcast_script(summaries)
    if not script:
        print("ç”Ÿæˆæ’­æŠ¥ç¨¿å¤±è´¥")
        return
    
    # ç¡®ä¿ä½¿ç”¨åŒä¸€ä¸ªæ—¶é—´æˆ³åˆ›å»ºç›®å½•
    podcast_dir = os.path.join(generator.podcasts_dir, timestamp)
    if not os.path.exists(podcast_dir):
        os.makedirs(podcast_dir)
    
    # ä¿å­˜æ–‡ç¨¿
    script_file = os.path.join(podcast_dir, 'script.txt')
    with open(script_file, 'w', encoding='utf-8') as f:
        f.write(script)
    
    # 4. ç”ŸæˆéŸ³é¢‘ - ä¼ å…¥ç›¸åŒçš„æ—¶é—´æˆ³
    audio_file = await generator.generate_audio(script, timestamp)
    
    # æ›´æ–°ç´¢å¼•æ–‡ä»¶ - ä½¿ç”¨ç›¸åŒçš„æ—¶é—´æˆ³
    podcast_data = {
        'id': timestamp,
        'date': datetime.now().strftime('%Y-%m-%d'),
        'title': f"å‡ºç‰ˆç”µå°æ’­æŠ¥ {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}",
        'summary': summaries[0]['title'] if summaries else "",
        'audio_path': f'/podcasts/{timestamp}/podcast.mp3',
        'script_path': f'/podcasts/{timestamp}/script.txt'
    }
    generator.update_podcast_index(podcast_data)
    
    # æ·»åŠ æ–‡ä»¶æ£€æŸ¥
    script_file = os.path.join(podcast_dir, 'script.txt')
    audio_file = os.path.join(podcast_dir, 'podcast.mp3')
    index_file = generator.index_file
    
    print("\næ–‡ä»¶æ£€æŸ¥:")
    print(f"æ’­å®¢ç›®å½•: {os.path.exists(podcast_dir)}")
    print(f"æ–‡ç¨¿æ–‡ä»¶: {os.path.exists(script_file)}")
    print(f"éŸ³é¢‘æ–‡ä»¶: {os.path.exists(audio_file)}")
    print(f"ç´¢å¼•æ–‡ä»¶: {os.path.exists(index_file)}")
    
    # æ‰“å°ç›®å½•å†…å®¹
    if os.path.exists(generator.web_dir):
        print("\nç½‘ç«™ç›®å½•å†…å®¹:")
        for root, dirs, files in os.walk(generator.web_dir):
            print(f"\n{root}:")
            for d in dirs:
                print(f"  ğŸ“ {d}")
            for f in files:
                print(f"  ğŸ“„ {f}")
    
    print("\nå¤„ç†å®Œæˆ!")
    print(f"æ–‡ä»¶å·²ä¿å­˜åœ¨: {os.path.join(generator.podcasts_dir, timestamp)}")

if __name__ == "__main__":
    asyncio.run(main()) 